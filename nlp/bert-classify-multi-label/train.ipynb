{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prerequisite: down data files from kaggle\n",
    "# https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data\n",
    "\n",
    "# References\n",
    "# 1. longformer: https://github.com/allenai/longformer\n",
    "# 2. longformer multilabel: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/hdddisk/anaconda3/envs/nlp/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/media/hdddisk/anaconda3/envs/nlp/lib/python3.11/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "2025-05-02 07:09:34.223710: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-02 07:09:34.242466: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from transformers import LongformerTokenizerFast, \\\n",
    "LongformerModel, LongformerConfig, Trainer, TrainingArguments, EvalPrediction, AutoTokenizer\n",
    "from transformers.models.longformer.modeling_longformer import LongformerPreTrainedModel, LongformerClassificationHead\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160 159\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6725d5a6391e5c77</td>\n",
       "      <td>Goal scored for Portugal \\n\\nThis could be mil...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28ea0d2c61db3137</td>\n",
       "      <td>My mistake someone was vandalizing the page so...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4de3d6b966b58ec7</td>\n",
       "      <td>Test card F music \\nCeefax music isn't the sam...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>af602c4c5f1b09bc</td>\n",
       "      <td>\":Meh, I guess I can live with either outcome,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9e412a7965873237</td>\n",
       "      <td>UV is my error, above. I'm told by Kimberly Ja...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>98a62f98dacc4553</td>\n",
       "      <td>Please use the articles discussion page.</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>af256c8248ec2d55</td>\n",
       "      <td>I can't make a modification to the infobox of ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>edc4512a1fb1bee5</td>\n",
       "      <td>Dont say that was a bad deletion,this article ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>02769bc92e9bd063</td>\n",
       "      <td>\"\\n\\n \"\"no you're wrong, it's perfect acceptab...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>9863d011e27322e7</td>\n",
       "      <td>:200001\\nWelcome to Wikipedia! We welcome your...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>160 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id                                       comment_text  \\\n",
       "0    6725d5a6391e5c77  Goal scored for Portugal \\n\\nThis could be mil...   \n",
       "1    28ea0d2c61db3137  My mistake someone was vandalizing the page so...   \n",
       "2    4de3d6b966b58ec7  Test card F music \\nCeefax music isn't the sam...   \n",
       "3    af602c4c5f1b09bc  \":Meh, I guess I can live with either outcome,...   \n",
       "4    9e412a7965873237  UV is my error, above. I'm told by Kimberly Ja...   \n",
       "..                ...                                                ...   \n",
       "155  98a62f98dacc4553           Please use the articles discussion page.   \n",
       "156  af256c8248ec2d55  I can't make a modification to the infobox of ...   \n",
       "157  edc4512a1fb1bee5  Dont say that was a bad deletion,this article ...   \n",
       "158  02769bc92e9bd063  \"\\n\\n \"\"no you're wrong, it's perfect acceptab...   \n",
       "159  9863d011e27322e7  :200001\\nWelcome to Wikipedia! We welcome your...   \n",
       "\n",
       "                 labels  \n",
       "0    [0, 0, 0, 0, 0, 0]  \n",
       "1    [0, 0, 0, 0, 0, 0]  \n",
       "2    [0, 0, 0, 0, 0, 0]  \n",
       "3    [0, 0, 0, 0, 0, 0]  \n",
       "4    [0, 0, 0, 0, 0, 0]  \n",
       "..                  ...  \n",
       "155  [0, 0, 0, 0, 0, 0]  \n",
       "156  [0, 0, 0, 0, 0, 0]  \n",
       "157  [0, 0, 0, 0, 0, 0]  \n",
       "158  [0, 0, 0, 0, 0, 0]  \n",
       "159  [0, 0, 0, 0, 0, 0]  \n",
       "\n",
       "[160 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the dataframe\n",
    "insults = pd.read_csv('/media/hdddisk/jigsaw-toxic-comment/train.csv')\n",
    "insults['labels'] = insults[insults.columns[2:]].values.tolist()\n",
    "insults = insults[['id','comment_text', 'labels']].reset_index(drop=True)\n",
    "\n",
    "train_size = 0.001\n",
    "train_dataset=insults.sample(frac=train_size,random_state=200)\n",
    "test_dataset=insults.drop(train_dataset.index).sample(frac=train_size,random_state=200).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "print(len(train_dataset), len(test_dataset))\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset['length'] = train_dataset['comment_text'].str.split().str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a Longformer for multilabel classification class\n",
    "class LongformerForMultiLabelSequenceClassification(LongformerPreTrainedModel):\n",
    "    \"\"\"\n",
    "    We instantiate a class of LongFormer adapted for a multilabel classification task. \n",
    "    This instance takes the pooled output of the LongFormer based model and passes it through a classification head. We replace the traditional Cross Entropy loss with a BCE loss that generate probabilities for all the labels that we feed into the model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(LongformerForMultiLabelSequenceClassification, self).__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.longformer = LongformerModel(config)\n",
    "        self.classifier = LongformerClassificationHead(config)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(self, input_ids=None, attention_mask=None, global_attention_mask=None, \n",
    "                token_type_ids=None, position_ids=None, inputs_embeds=None, \n",
    "                labels=None):\n",
    "        \n",
    "        # create global attention on sequence, and a global attention token on the `s` token\n",
    "        # the equivalent of the CLS token on BERT models. This is taken care of by HuggingFace\n",
    "        # on the LongformerForSequenceClassification class\n",
    "        if global_attention_mask is None:\n",
    "            global_attention_mask = torch.zeros_like(input_ids)\n",
    "            global_attention_mask[:, 0] = 1\n",
    "        \n",
    "        # pass arguments to longformer model\n",
    "        outputs = self.longformer(\n",
    "            input_ids = input_ids,\n",
    "            attention_mask = attention_mask,\n",
    "            global_attention_mask = global_attention_mask,\n",
    "            token_type_ids = token_type_ids,\n",
    "            position_ids = position_ids\n",
    "        )\n",
    "        \n",
    "        # if specified the model can return a dict where each key corresponds to the output of a\n",
    "        # LongformerPooler output class. In this case we take the last hidden state of the sequence\n",
    "        # which will have the shape (batch_size, sequence_length, hidden_size). \n",
    "        sequence_output = outputs['last_hidden_state']\n",
    "        \n",
    "        # pass the hidden states through the classifier to obtain thee logits\n",
    "        logits = self.classifier(sequence_output)\n",
    "        outputs = (logits,) + outputs[2:]\n",
    "        if labels is not None:\n",
    "            loss_fct = BCEWithLogitsLoss()\n",
    "            labels = labels.float()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), \n",
    "                            labels.view(-1, self.num_labels))\n",
    "            outputs = (loss,) + outputs\n",
    "        \n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a class that will handle the data\n",
    "class Data_Processing(object):\n",
    "    def __init__(self, tokenizer, id_column, text_column, label_column):\n",
    "        \n",
    "        # define the text column from the dataframe\n",
    "        self.text_column = text_column.tolist()\n",
    "    \n",
    "        # define the label column and transform it to list\n",
    "        self.label_column = label_column\n",
    "        \n",
    "        # define the id column and transform it to list\n",
    "        self.id_column = id_column.tolist()\n",
    "        \n",
    "    \n",
    "# iter method to get each element at the time and tokenize it using bert        \n",
    "    def __getitem__(self, index):\n",
    "        comment_text = str(self.text_column[index])\n",
    "        comment_text = \" \".join(comment_text.split())\n",
    "        # encode the sequence and add padding\n",
    "        inputs = tokenizer.encode_plus(comment_text,\n",
    "                                       add_special_tokens = True,\n",
    "                                       max_length= 3048,\n",
    "                                       padding = 'max_length',\n",
    "                                       return_attention_mask = True,\n",
    "                                       truncation = True,\n",
    "                                       return_tensors='pt')\n",
    "        input_ids = inputs['input_ids']\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        \n",
    "        labels_ = torch.tensor(self.label_column[index], dtype=torch.float)\n",
    "        id_ = self.id_column[index]\n",
    "        return {'input_ids':input_ids[0], 'attention_mask':attention_mask[0], \n",
    "                'labels':labels_, 'id_':id_}\n",
    "  \n",
    "    def __len__(self):\n",
    "        return len(self.text_column) \n",
    "\n",
    "batch_size = 2\n",
    "# create a class to process the training and test data\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'allenai/longformer-base-4096', \n",
    "    padding = 'max_length',\n",
    "    truncation=True, \n",
    "    max_length = 3048\n",
    ")\n",
    "training_data = Data_Processing(\n",
    "    tokenizer, \n",
    "    train_dataset['id'], \n",
    "    train_dataset['comment_text'], \n",
    "    train_dataset['labels']\n",
    ")\n",
    "\n",
    "test_data =  Data_Processing(\n",
    "    tokenizer, \n",
    "    test_dataset['id'], \n",
    "    test_dataset['comment_text'], \n",
    "    test_dataset['labels']\n",
    ")\n",
    "\n",
    "# use the dataloaders class to load the data\n",
    "dataloaders_dict = {\n",
    "    'train': DataLoader(training_data, batch_size=batch_size, shuffle=True, num_workers=4),\n",
    "    'val': DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "}\n",
    "\n",
    "dataset_sizes = {\n",
    "    'train':len(training_data),\n",
    "    'val':len(test_data)\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LongformerForMultiLabelSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = LongformerForMultiLabelSequenceClassification.from_pretrained(\n",
    "    'allenai/longformer-base-4096',\n",
    "    gradient_checkpointing=False,\n",
    "    attention_window = 512,\n",
    "    num_labels = 6,\n",
    "    return_dict=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "    \n",
    "def multi_label_metrics(predictions, labels):\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_true = labels\n",
    "    y_pred[np.where(probs >= 0.5)] = 1\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    # define dictionary of metrics to return\n",
    "    metrics = {\n",
    "        'f1': f1_micro_average,\n",
    "        'roc_auc': roc_auc,\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "# Use the aux EvalPrediction class to obtain prediction labels\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, \n",
    "            tuple) else p.predictions\n",
    "    result = multi_label_metrics(\n",
    "        predictions=preds, \n",
    "        labels=p.label_ids)\n",
    "    return result\n",
    "\n",
    "# define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = '/media/hdddisk/bert-classify-multi-label/results',\n",
    "    num_train_epochs=5,                # Number of training epochs\n",
    "    per_device_train_batch_size=1,     # Batch size for training\n",
    "    per_device_eval_batch_size=1,      # Batch size for evaluation\n",
    "    eval_strategy=\"steps\",             # Evaluate every epoch = \"epoch\"\n",
    "    eval_steps=100,                    # Number of steps between evaluations\n",
    "    save_strategy=\"steps\",             # Save model every epoch\n",
    "    save_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    # warmup_steps=100,                   # Number of warmup steps for learning rate scheduler\n",
    "    learning_rate = 2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps = 20,\n",
    "    logging_dir='/media/hdddisk/bert-classify-multi-label/logs',\n",
    "    run_name = 'longformer_multilabel_paper_trainer_3048_2e5',\n",
    "    report_to=\"mlflow\",                # Enable logging to MLflow\n",
    ")\n",
    "# instantiate the trainer class and check for available devices\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=training_data,\n",
    "    eval_dataset=test_data,\n",
    "    compute_metrics = compute_metrics,\n",
    "    #data_collator = Data_Processing(),\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/5', creation_time=1745557944529, experiment_id='5', last_update_time=1745557944529, lifecycle_stage='active', name='bert-classify-multi-label-jigsaw-toxic-comment', tags={}>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "mlflow.set_tracking_uri(\"https://mlflow.yellowday.day\")\n",
    "mlflow.set_experiment(\"bert-classify-multi-label-jigsaw-toxic-comment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name='longformer-multilabel-small-v2') as run:\n",
    "    trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
