# Train 1 train/valid split - This is the heart of the pipeline

from preproc import create_and_apply_num_scaler
from dls import create_dataloaders
from model import setup_model
from training_loop import train_model
from utils import create_train_log_df, set_device
from storage import (
    save_model,
    save_model_checkpoint_map,
    save_train_log,
)
from config import CFG
from mlflow_utils import log_config_params

import datetime
import mlflow
import gc 
import torch


def train_1_fold(
    preproc_data_df,
    label_encoder,
    val_fold,
    base_model_id,
    CFG: CFG,
    val_fold_list=None,
):
    """Train multiple epochs for 1 single validation fold; loop this e.g. 5 times for 5-fold CV

    Inputs:
    - preproc_data_df               - the preprocessed dataframe after preproc.preprocess
    - label_encoder                 - the trained label encoder from preproc.encode_labels
    - val_fold                      - the index of the validation fold (e.g. 0-4 for 5-fold CV)
    - base_model_id                 - CFG.model_name if not None, or generated by get_base_model_id from CFG.checkpoint and pipeline start utc dt
    - val_folds_list                - CFG.val_folds in list form, to determine how we name the output model
    - val_folds                     - CFG.val_folds; either list or number of folds;
    - item_col_name                 - the original name of the item text column
    - max_len                       - CFG max string length
    - bs                            - CFG batch size
    - gradient_accumulation_steps   - CFG number of steps for gradient accumulation
    - checkpoint                    - CFG checkpoint name
    - lr                            - CFG learning rate
    - n_epochs                      - CFG number of training epochs
    - optimizer_name                - CFG optimizer name
    - metric                        - CFG performance metric
    - loss                          - CFG loss function name
    - save_model_flag               - CFG do we save the model?
    - proj_dir                      - CFG project directory

    Returns:
    - train_log_df                  - pd Dataframe, training performance logs and meta information
    - oof_df                        - pd Dataframe, out-of-fold predictions on validation fold with some meta info

    Also:
    - saves trained model to temp directory or project directory (if save_model_flag == True)
    """

    # if we're training more than a single fold overall, append the fold number to the model_id
    model_id = (
        "__".join([base_model_id, str(val_fold)])
        if len(val_fold_list) > 1
        else base_model_id
    )

    # time keeping
    fold_start_dt = datetime.datetime.utcnow()

    print(
        f"\n------------- FOLD {val_fold} ({CFG.val_folds.index(val_fold) + 1}/{len(CFG.val_folds) if isinstance(CFG.val_folds, list) else CFG.val_folds}) -------------\n"
    )

    # Scale num inputs
    if CFG.num_col_names and CFG.scale_num_cols and CFG.build_custom_model_type == "num_metric":
        preproc_data_df, scaler = create_and_apply_num_scaler(
            preproc_data_df, CFG, val_fold, model_id
        )

    # Create Dataloaders
    print("Creating Dataloaders...")
    dl_start_dt = datetime.datetime.utcnow()
    train_dl, eval_dl, auto_max_len, tokenizer = create_dataloaders(preproc_data_df, val_fold, CFG)
    print(
        f"Dataloaders created. Time elapsed: {datetime.datetime.utcnow() - dl_start_dt}"
    )

    # Device management
    device = set_device(CFG.device)
    print(f"Using device {device}")

    # Start MLFlow Run and log params
    if CFG.mlflow and CFG._experiment_id:
        if mlflow.active_run():
            mlflow.end_run()
        if CFG.run_description:
            mlflow.start_run(run_name=model_id, experiment_id=CFG._experiment_id, description=CFG.run_description)
        else:
            mlflow.start_run(run_name=model_id, experiment_id=CFG._experiment_id)
        log_config_params(CFG, {"fold": val_fold, "auto_max_len": auto_max_len, "device": str(device)})

    # Model setup
    print("Configuring model...")
    mod_start_dt = datetime.datetime.utcnow()
    model, optimizer, lr_scheduler = setup_model(train_dl, device, CFG)
    print(
        f"Model configured. Time elapsed: {datetime.datetime.utcnow() - mod_start_dt}"
    )

    # Training loop
    print("Training model...")
    train_start_dt = datetime.datetime.utcnow()
    trained_model, train_results, embeddings_eval, embeddings_train = train_model(
        model,
        optimizer,
        lr_scheduler,
        label_encoder,
        train_dl,
        eval_dl,
        device,
        CFG,
    )
    print(
        f"Training loop complete. Time elapsed: {datetime.datetime.utcnow() - train_start_dt}"
    )

    # End mlflow run
    if CFG.mlflow and CFG._experiment_id and CFG.end_last_run:
        mlflow.end_run()

    # Create training log df
    # this keeps track of training performance in a standardised way, to allow model comparison analysis
    train_log_df = create_train_log_df(
        train_results, base_model_id, model_id, val_fold, auto_max_len, CFG
    )

    # Save Model and update model_checkpoint_map json
    if CFG.save_flag:
        save_model(trained_model, model_id, CFG)
        save_model_checkpoint_map(model_id, CFG.checkpoint, CFG.proj_dir)
    elif CFG.temp_flag:
        save_model(trained_model, model_id, CFG, temp=True)

    # Save logs
    save_train_log(train_log_df, model_id, CFG)

    print(
        f"Fold {val_fold} loop complete. Time elapsed: {datetime.datetime.utcnow() - fold_start_dt}"
    )

    # Create result dictionary
    fold_results = {
        "model_id": model_id,
        "fold": int(val_fold),
        "log": train_log_df,
    }

    if CFG.return_embeddings:
        # Create train_df and test_df so we can match up embeddings
        train_df = preproc_data_df[preproc_data_df.fold != val_fold].reset_index(drop=True)
        test_df = preproc_data_df.loc[preproc_data_df.fold == val_fold].reset_index(
            drop=True
        )
        fold_results.update(
            {
                "train_embeddings": embeddings_train,
                "train_df": train_df,
                "test_embeddings": embeddings_eval,
                "test_df": test_df,
            }
        )

    if CFG.return_artifacts:
        fold_results["model"] = trained_model
        fold_results["tokenizer"] = tokenizer

        if CFG.num_col_names and CFG.scale_num_cols and CFG.build_custom_model_type == "num_metric":
            fold_results["scaler"] = scaler
    
    else:
        del trained_model
        del tokenizer

        if CFG.num_col_names and CFG.scale_num_cols and CFG.build_custom_model_type == "num_metric":
            fold_results["scaler"] = scaler

    # Clean up 
    print("Cleaning up...")
    gc.collect()
    torch.cuda.empty_cache()
    del label_encoder
        
    return fold_results
