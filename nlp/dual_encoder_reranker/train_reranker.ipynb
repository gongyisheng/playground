{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dual Encoder + Reranker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reranker training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from sentence_transformers.cross_encoder import (\n",
    "    CrossEncoder,\n",
    "    CrossEncoderModelCardData,\n",
    "    CrossEncoderTrainer,\n",
    "    CrossEncoderTrainingArguments,\n",
    ")\n",
    "from sentence_transformers.cross_encoder.losses import BinaryCrossEntropyLoss\n",
    "from sentence_transformers.util import mine_hard_negatives\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['query', 'answer'],\n",
      "    num_rows: 100231\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "train_dataset = load_dataset(\"sentence-transformers/natural-questions\", split=\"train\")\n",
    "eval_dataset = load_dataset(\"sentence-transformers/natural-questions\", split=\"validation\")\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at answerdotai/ModernBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-05-14 15:53:32 - Use pytorch device: mps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model max length: 8192\n",
      "Model num labels: 1\n"
     ]
    }
   ],
   "source": [
    "# Set the log level to INFO to get more information\n",
    "logging.basicConfig(format=\"%(asctime)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\", level=logging.INFO)\n",
    "\n",
    "# You can specify any Hugging Face pre-trained model here, for example, bert-base-uncased, roberta-base, xlm-roberta-base\n",
    "model_name = \"answerdotai/ModernBERT-base\"\n",
    "train_batch_size = 8\n",
    "num_epochs = 1\n",
    "num_hard_negatives = 5  # How many hard negatives should be mined for each question-answer pair\n",
    "output_dir = (\n",
    "    \"/data/yisheng/reranker_2025_05_13/\" + model_name.replace(\"/\", \"-\") + \"-\" + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    ")\n",
    "\n",
    "# Load a model to train/finetune\n",
    "model = CrossEncoder(model_name) # num_labels=1 is for rerankers\n",
    "print(\"Model max length:\", model.max_length)\n",
    "print(\"Model num labels:\", model.num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-14 15:53:33 - Load pretrained SentenceTransformer: sentence-transformers/static-retrieval-mrl-en-v1\n",
      "2025-05-14 15:53:34 - The `margin` parameter is deprecated. Use the `absolute_margin` and/or `relative_margin` parameter instead. Setting `absolute_margin` to `0`.\n",
      "Batches: 100%|██████████| 19/19 [00:09<00:00,  1.95it/s]\n",
      "Batches: 100%|██████████| 25/25 [00:01<00:00, 15.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# mine hard negatives\n",
    "# The success of training reranker models often depends on the quality of the negatives\n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/static-retrieval-mrl-en-v1\", device=\"cpu\")\n",
    "hard_train_dataset = mine_hard_negatives(\n",
    "    train_dataset,\n",
    "    embedding_model,\n",
    "    num_negatives=num_hard_negatives,  # How many negatives per question-answer pair\n",
    "    range_min=0,  # Skip the x most similar samples\n",
    "    range_max=20,  # Consider only the x most similar samples\n",
    "    margin=0,  # Similarity between query and negative samples should be x lower than query-positive similarity\n",
    "    sampling_strategy=\"top\",  # Randomly sample negatives from the range\n",
    "    batch_size=4096,  # Use a batch size of 4096 for the embedding model\n",
    "    output_format=\"labeled-pair\",  # The output format is (query, passage, label), as required by BinaryCrossEntropyLoss\n",
    "    use_faiss=False,  # Using FAISS is recommended to keep memory usage low (pip install faiss-gpu or pip install faiss-cpu)\n",
    ")\n",
    "print(hard_train_dataset)\n",
    "print(hard_train_dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import CrossEncoder\n",
    "from sentence_transformers.cross_encoder.losses import CachedMultipleNegativesRankingLoss\n",
    "\n",
    "# Load a model to train/finetune\n",
    "model = CrossEncoder(\"xlm-roberta-base\", num_labels=1) # num_labels=1 is for rerankers\n",
    "\n",
    "# Initialize the CachedMultipleNegativesRankingLoss, which requires pairs of\n",
    "# related texts or triplets\n",
    "loss = CachedMultipleNegativesRankingLoss(model)\n",
    "\n",
    "# Load an example training dataset that works with our loss function:\n",
    "train_dataset = load_dataset(\"sentence-transformers/gooaq\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "from sentence_transformers.cross_encoder import CrossEncoderTrainingArguments\n",
    "\n",
    "args = CrossEncoderTrainingArguments(\n",
    "    # Required parameter:\n",
    "    output_dir=\"models/reranker-MiniLM-msmarco-v1\",\n",
    "    # Optional training parameters:\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    fp16=True,  # Set to False if you get an error that your GPU can't run on FP16\n",
    "    bf16=False,  # Set to True if you have a GPU that supports BF16\n",
    "    batch_sampler=BatchSamplers.NO_DUPLICATES,  # losses that use \"in-batch negatives\" benefit from no duplicates\n",
    "    # Optional tracking/debugging parameters:\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    "    run_name=\"reranker-MiniLM-msmarco-v1\",  # Will be used in W&B if `wandb` is installed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import CrossEncoder\n",
    "from sentence_transformers.cross_encoder.evaluation import CrossEncoderCorrelationEvaluator\n",
    "\n",
    "# Load a model\n",
    "model = CrossEncoder(\"cross-encoder/stsb-TinyBERT-L4\")\n",
    "\n",
    "# Load the STSB dataset (https://huggingface.co/datasets/sentence-transformers/stsb)\n",
    "eval_dataset = load_dataset(\"sentence-transformers/stsb\", split=\"validation\")\n",
    "pairs = list(zip(eval_dataset[\"sentence1\"], eval_dataset[\"sentence2\"]))\n",
    "\n",
    "# Initialize the evaluator\n",
    "dev_evaluator = CrossEncoderCorrelationEvaluator(\n",
    "    sentence_pairs=pairs,\n",
    "    scores=eval_dataset[\"score\"],\n",
    "    name=\"sts_dev\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
