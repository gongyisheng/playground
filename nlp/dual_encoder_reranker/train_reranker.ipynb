{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dual Encoder + Reranker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reranker training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import mine_hard_negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "train_dataset = load_dataset(\"sentence-transformers/natural-questions\", split=\"train\")\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mine hard negatives\n",
    "# The success of training reranker models often depends on the quality of the negatives\n",
    "\n",
    "# Load the GooAQ dataset: https://huggingface.co/datasets/sentence-transformers/gooaq\n",
    "train_dataset = load_dataset(\"sentence-transformers/gooaq\", split=\"train\").select(range(100_000))\n",
    "print(train_dataset)\n",
    "\n",
    "# Mine hard negatives using a very efficient embedding model\n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/static-retrieval-mrl-en-v1\", device=\"cpu\")\n",
    "hard_train_dataset = mine_hard_negatives(\n",
    "    train_dataset,\n",
    "    embedding_model,\n",
    "    num_negatives=5,          # How many negatives per question-answer pair\n",
    "    range_min=10,             # Skip the x most similar samples\n",
    "    range_max=100,            # Consider only the x most similar samples\n",
    "    max_score=0.8,            # Only consider samples with a similarity score of at most x\n",
    "    margin=0.1,               # Similarity between query and negative samples should be x lower than query-positive similarity\n",
    "    sampling_strategy=\"top\",  # Randomly sample negatives from the range\n",
    "    batch_size=4096,          # Use a batch size of 4096 for the embedding model\n",
    "    output_format=\"labeled-pair\",  # The output format is (query, passage, label), as required by BinaryCrossEntropyLoss\n",
    "    use_faiss=True,           # Using FAISS is recommended to keep memory usage low (pip install faiss-gpu or pip install faiss-cpu)\n",
    ")\n",
    "print(hard_train_dataset)\n",
    "print(hard_train_dataset[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import CrossEncoder\n",
    "from sentence_transformers.cross_encoder.losses import CachedMultipleNegativesRankingLoss\n",
    "\n",
    "# Load a model to train/finetune\n",
    "model = CrossEncoder(\"xlm-roberta-base\", num_labels=1) # num_labels=1 is for rerankers\n",
    "\n",
    "# Initialize the CachedMultipleNegativesRankingLoss, which requires pairs of\n",
    "# related texts or triplets\n",
    "loss = CachedMultipleNegativesRankingLoss(model)\n",
    "\n",
    "# Load an example training dataset that works with our loss function:\n",
    "train_dataset = load_dataset(\"sentence-transformers/gooaq\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "from sentence_transformers.cross_encoder import CrossEncoderTrainingArguments\n",
    "\n",
    "args = CrossEncoderTrainingArguments(\n",
    "    # Required parameter:\n",
    "    output_dir=\"models/reranker-MiniLM-msmarco-v1\",\n",
    "    # Optional training parameters:\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    fp16=True,  # Set to False if you get an error that your GPU can't run on FP16\n",
    "    bf16=False,  # Set to True if you have a GPU that supports BF16\n",
    "    batch_sampler=BatchSamplers.NO_DUPLICATES,  # losses that use \"in-batch negatives\" benefit from no duplicates\n",
    "    # Optional tracking/debugging parameters:\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    "    run_name=\"reranker-MiniLM-msmarco-v1\",  # Will be used in W&B if `wandb` is installed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import CrossEncoder\n",
    "from sentence_transformers.cross_encoder.evaluation import CrossEncoderCorrelationEvaluator\n",
    "\n",
    "# Load a model\n",
    "model = CrossEncoder(\"cross-encoder/stsb-TinyBERT-L4\")\n",
    "\n",
    "# Load the STSB dataset (https://huggingface.co/datasets/sentence-transformers/stsb)\n",
    "eval_dataset = load_dataset(\"sentence-transformers/stsb\", split=\"validation\")\n",
    "pairs = list(zip(eval_dataset[\"sentence1\"], eval_dataset[\"sentence2\"]))\n",
    "\n",
    "# Initialize the evaluator\n",
    "dev_evaluator = CrossEncoderCorrelationEvaluator(\n",
    "    sentence_pairs=pairs,\n",
    "    scores=eval_dataset[\"score\"],\n",
    "    name=\"sts_dev\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
