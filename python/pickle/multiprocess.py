import io
from pickle import Unpickler
from multiprocessing.reduction import ForkingPickler
import pybase64

class MultiprocessingSerializer:
    @staticmethod
    def serialize(obj, output_str: bool = False):
        """
        Serialize a Python object using ForkingPickler.

        Args:
            obj: The object to serialize.
            output_str (bool): If True, return a base64-encoded string instead of raw bytes.

        Returns:
            bytes or str: The serialized object.
        """
        buf = io.BytesIO()
        ForkingPickler(buf).dump(obj)
        buf.seek(0)
        output = buf.read() # the output is OS-level shared memory identifier

        if output_str:
            # Convert bytes to base64-encoded string
            output = pybase64.b64encode(output).decode("utf-8")

        return output

    @staticmethod
    def deserialize(data):
        """
        Deserialize a previously serialized object.

        Args:
            data (bytes or str): The serialized data, optionally base64-encoded.

        Returns:
            The deserialized Python object.
        """
        if isinstance(data, str):
            # Decode base64 string to bytes
            data = pybase64.b64decode(data, validate=True)

        return Unpickler(io.BytesIO(data)).load()


def test():
    # example 1: serialize to bytes
    data = {"key": "value", "number": 42, "list": [1, 2, 3]}
    serialized = MultiprocessingSerializer.serialize(data)
    deserialized = MultiprocessingSerializer.deserialize(serialized)
    assert deserialized == data
    print("bytes serialization: passed")

    # example 2: serialize to base64 string
    serialized_str = MultiprocessingSerializer.serialize(data, output_str=True)
    deserialized = MultiprocessingSerializer.deserialize(serialized_str)
    assert deserialized == data
    print("string serialization: passed")


if __name__ == "__main__":
    test()