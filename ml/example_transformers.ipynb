{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref: \n",
    "# doc: https://huggingface.co/docs/transformers/en/main_classes/tokenizer\n",
    "# post: https://juejin.cn/post/7365704546100396084"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 31414, 623, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 2387, 766, 16, 1560, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 5625, 16, 18862, 46836, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode text\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = 'distilbert/distilroberta-base'\n",
    "max_length = 32\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "text = [\"Hello World\", \"My name is Tom\", \"Today is wednesday\"]\n",
    "\n",
    "encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=max_length)\n",
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[max_length] shape: (3, 16)\n",
      "[longest] shape: (3, 6)\n"
     ]
    }
   ],
   "source": [
    "# use different padding strategies\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = 'distilbert/distilroberta-base'\n",
    "max_length = 16\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def print_shape(encoding, label):\n",
    "    shape = (len(encoding[\"input_ids\"]), len(encoding[\"input_ids\"][0]))\n",
    "    print(f\"[{label}] shape:\", shape)\n",
    "\n",
    "text = [\"Hello World\", \"My name is Tom\", \"Today is wednesday\"]\n",
    "\n",
    "encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=max_length)\n",
    "print_shape(encoding, \"max_length\")\n",
    "\n",
    "encoding = tokenizer(text, padding=\"longest\", truncation=True, max_length=max_length)\n",
    "print_shape(encoding, \"longest\")\n",
    "\n",
    "encoding = tokenizer(text, padding=False, truncation=True, max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 31414, 623, 2]]\n",
      "right: [[0, 31414, 623, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "# padding side: left or right\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = 'distilbert/distilroberta-base'\n",
    "max_length = 16\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "text = [\"Hello World\"]\n",
    "\n",
    "encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=max_length, padding_side='left')\n",
    "print(\"left:\", encoding['input_ids'])\n",
    "\n",
    "encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=max_length, padding_side='right')\n",
    "print(\"right:\", encoding['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 713, 16, 41, 1246, 9, 10, 2788, 14, 16, 1341, 8787, 8, 1411, 1684, 5, 4532, 1220, 8135, 5933, 13, 1209, 11760, 3092, 4, 1773, 52, 64, 75, 3993, 42, 1445, 2788, 88, 5, 1421, 396, 43064, 1295, 24, 6, 52, 3253, 43064, 1258, 7, 1306, 14, 5, 2788, 10698, 624, 5, 1421, 18, 4532, 8135, 5933, 3000, 4, 2]\n",
      "[0, 713, 16, 41, 1246, 9, 10, 2]\n"
     ]
    }
   ],
   "source": [
    "# truncation\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = 'distilbert/distilroberta-base'\n",
    "max_length = 8\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "text = (\"This is an example of a text that is quite lengthy and goes beyond \"\n",
    "        \"the maximum allowed input length for Qwen models. Since we can't \"\n",
    "        \"feed this entire text into the model without truncating it, we apply \"\n",
    "        \"truncation to ensure that the text fits within the model's maximum input \"\n",
    "        \"length limit.\")\n",
    "\n",
    "encoding = tokenizer(text, truncation=False, max_length=max_length)\n",
    "print(encoding[\"input_ids\"])\n",
    "\n",
    "encoding = tokenizer(text, truncation=True, max_length=max_length)\n",
    "print(encoding[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# return tensorflow / pytorch / numpy tensor\n",
    "# return_tensors (str or TensorType, optional) — If set, will return tensors instead of list of python integers. Acceptable values are:\n",
    "# 'tf': Return TensorFlow tf.constant objects.\n",
    "# 'pt': Return PyTorch torch.Tensor objects.\n",
    "# 'np': Return Numpy np.ndarray objects.\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = 'distilbert/distilroberta-base'\n",
    "max_length = 16\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "text = [\"Hello World\"]\n",
    "\n",
    "encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors='tf')\n",
    "print(type(encoding[\"input_ids\"]))\n",
    "\n",
    "encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors='pt')\n",
    "print(type(encoding[\"input_ids\"]))\n",
    "\n",
    "encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors='np')\n",
    "print(type(encoding[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
      "{'input_ids': [101, 1188, 1110, 1103, 1148, 1413, 106, 102, 1188, 1110, 1103, 1248, 1413, 106, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['[CLS]', 'This', 'is', 'the', 'first', 'line', '!', '[SEP]', 'This', 'is', 'the', 'second', 'line', '!', '[SEP]']\n",
      "[None, 0, 1, 2, 3, 4, 5, None, 0, 1, 2, 3, 4, 5, None]\n",
      "[None, 0, 1, 2, 3, 4, 5, None, 0, 1, 2, 3, 4, 5, None]\n",
      "[None, 0, 0, 0, 0, 0, 0, None, 1, 1, 1, 1, 1, 1, None]\n",
      "True\n",
      "1\n",
      "8\n",
      "CharSpan(start=8, end=11)\n",
      "CharSpan(start=8, end=11)\n",
      "0\n",
      "1\n",
      "2\n",
      "2\n",
      "CharSpan(start=12, end=17)\n",
      "CharSpan(start=12, end=18)\n",
      "TokenSpan(start=1, end=2)\n",
      "TokenSpan(start=8, end=9)\n"
     ]
    }
   ],
   "source": [
    "# fast tokenizer & result checking\n",
    "from transformers import BertTokenizerFast\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-cased\")\n",
    "result = tokenizer(\"This is the first line!\", \"This is the second line!\") \n",
    "\n",
    "################# check result ###########\n",
    "print(type(result))\n",
    "# <class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
    "print(result)\n",
    "# {'input_ids': [101, 1188, 1110, 1103, 1148, 1413, 106, 102, 1188, 1110, 1103, 1248, 1413, 106, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
    "print(result.tokens()) \n",
    "# ['[CLS]', 'This', 'is', 'the', 'first', 'line', '!', '[SEP]', 'This', 'is', 'the', 'second', 'line', '!', '[SEP]']\n",
    "print(result.words()) # 每个 token 属于该句子的第几个单词\n",
    "# [None, 0, 1, 2, 3, 4, 5, None, 0, 1, 2, 3, 4, 5, None]\n",
    "print(result.word_ids()) # 每个 token 属于该句子的第几个单词\n",
    "# [None, 0, 1, 2, 3, 4, 5, None, 0, 1, 2, 3, 4, 5, None]\n",
    "print(result.sequence_ids()) # 每个 token 属于第几个句子\n",
    "# [None, 0, 0, 0, 0, 0, 0, None, 1, 1, 1, 1, 1, 1, None]\n",
    "print(result.is_fast) # 是否 fast tokenizer\n",
    "# True\n",
    "################## convert ################\n",
    "print(result.char_to_token(3)) # 第一个句子第三个字符属于第几个 token \n",
    "# 1\n",
    "print(result.char_to_token(3, sequence_index=1)) # 第二个句子第三个字符属于第几个 token \n",
    "# 8\n",
    "\n",
    "print(result.token_to_chars(3)) # 第三个 token 在原始句子中的区间\n",
    "# CharSpan(start=8, end=11)\n",
    "print(result.token_to_chars(10)) # 第十个 token 在原始句子中的区间\n",
    "# CharSpan(start=8, end=11) \n",
    "\n",
    "print(result.token_to_sequence(3)) # 第三个 token 是第一个句子还是第二个句子\n",
    "# 0\n",
    "print(result.token_to_sequence(10)) # 第十个 token 是第一个句子还是第二个句子\n",
    "# 1\n",
    "\n",
    "print(result.token_to_word(3)) # 第三个 token 是在该句子中的第几个单词\n",
    "# 2\n",
    "print(result.token_to_word(10)) # 第十个 token 是在该句子中的第几个单词\n",
    "# 2\n",
    "\n",
    "print(result.word_to_chars(3)) # 第一个句子第三个单词位于原始句子中的区间 \n",
    "# CharSpan(start=12, end=17)\n",
    "print(result.word_to_chars(3, sequence_index=1)) # 第二个句子第三个单词位于原始句子中的区间\n",
    "# CharSpan(start=12, end=18)\n",
    "\n",
    "print(result.word_to_tokens(0)) # 第一个句子第一个单词对应的 token 区间\n",
    "# TokenSpan(start=1, end=2)\n",
    "print(result.word_to_tokens(0, sequence_index=1)) # 第二个句子第一个单词对应的 token 区间\n",
    "# TokenSpan(start=8, end=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
