{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytorch basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6032, 0.3480, 0.1462],\n",
      "        [0.7424, 0.0804, 0.2987],\n",
      "        [0.4658, 0.3211, 0.5640]])\n",
      "Shape: torch.Size([3, 3])\n",
      "Data type: torch.float32\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# tensor\n",
    "\n",
    "# create a (3x3) tensor of random values between 0 and 1\n",
    "x = torch.rand(3, 3)  \n",
    "print(x)\n",
    "\n",
    "# create a tensor filled with zeros or ones\n",
    "zeros = torch.zeros(2, 2)\n",
    "ones = torch.ones(2, 2)\n",
    "\n",
    "# create tensors from Python lists\n",
    "tensor_from_list = torch.tensor([[1, 2], [3, 4]])\n",
    "\n",
    "# tensor attributes\n",
    "print(\"Shape:\", x.shape)  # Dimensions of the tensor\n",
    "print(\"Data type:\", x.dtype)  # Data type of tensor elements\n",
    "print(\"Device:\", x.device)  # Device (CPU or GPU) where the tensor resides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 9])\n",
      "tensor([10, 18])\n",
      "tensor([[0.4629, 0.1539, 0.4662, 0.5405],\n",
      "        [1.0802, 0.8159, 1.4383, 0.9259]])\n",
      "tensor([[0.4629, 0.1539, 0.4662, 0.5405],\n",
      "        [1.0802, 0.8159, 1.4383, 0.9259]])\n",
      "tensor([[0.7427, 0.3080, 0.4054],\n",
      "        [0.4698, 0.5344, 0.5900],\n",
      "        [0.3126, 0.7473, 0.5904]])\n",
      "tensor([[0.7427, 0.3080, 0.4054, 0.4698, 0.5344, 0.5900, 0.3126, 0.7473, 0.5904]])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "torch.Size([2, 3])\n",
      "tensor([[[1, 2],\n",
      "         [3, 4]],\n",
      "\n",
      "        [[5, 6],\n",
      "         [7, 8]]])\n",
      "torch.Size([2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "# tensor operations\n",
    "\n",
    "# element-wise addition\n",
    "a = torch.tensor([2, 3])\n",
    "b = torch.tensor([5, 6])\n",
    "print(a + b)  # [7, 9, 11]\n",
    "\n",
    "# element-wise multiplication\n",
    "print(a * b)  # [10, 18, 28]\n",
    "\n",
    "# matrix multiplication\n",
    "c = torch.rand(2, 3)\n",
    "d = torch.rand(3, 4)\n",
    "## dot product\n",
    "print(torch.mm(c, d))  # Result is a (2x4) matrix\n",
    "print(torch.matmul(c, d)) # Same result as above\n",
    "\n",
    "# reshaping tensors\n",
    "x = torch.rand(3, 3)\n",
    "x_reshaped = x.view(1, 9)  # Reshape to (1x9)\n",
    "print(x)\n",
    "print(x_reshaped) # flattened tensor\n",
    "\n",
    "# stack\n",
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([4, 5, 6])\n",
    "stacked_1d = torch.stack([a, b])\n",
    "print(stacked_1d)\n",
    "print(stacked_1d.shape)\n",
    "\n",
    "c = torch.tensor([[1, 2], [3, 4]])\n",
    "d = torch.tensor([[5, 6], [7, 8]])\n",
    "stacked_2d = torch.stack([c, d])\n",
    "print(stacked_2d)\n",
    "print(stacked_2d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n",
      "Model output: tensor([[0.2039]])\n"
     ]
    }
   ],
   "source": [
    "# gradient tracking\n",
    "\n",
    "# the function is y=f(x), how y changes with respect to x\n",
    "# gradient is dy/dx\n",
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "y = x + 2\n",
    "z = y * y * 3 \n",
    "out = z.mean()\n",
    "\n",
    "# Gradient of out\n",
    "# out = z/4 = y*y*3/4 = (x+2)*(x+2)*3/4\n",
    "# gradient = dy/dx = 3/4*(2x+4), set x=1, get gradient = 4.5\n",
    "out.backward()\n",
    "print(x.grad)\n",
    "\n",
    "# use torch.no_grad() to disable gradient tracking\n",
    "model = torch.nn.Linear(10, 1)\n",
    "input_data = torch.randn(1, 10)\n",
    "with torch.no_grad():\n",
    "    output = model(input_data)\n",
    "print(\"Model output:\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 10])\n",
      "torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "# torch.nn, neural networks\n",
    "\n",
    "# neural network layers\n",
    "# fully connected (dense) layers\n",
    "# torch.nn.Linear(in_features, out_features)\n",
    "# Input (x) → Linear Layer → Output (y = xW^T + b)\n",
    "layer = torch.nn.Linear(1,1) # Linear layer with 1 input and 1 output\n",
    "layer = torch.nn.Linear(10, 5) # Linear layer with 10 inputs and 5 outputs\n",
    "# weights\n",
    "# weights shape should be (out_features, in_features)\n",
    "# each of the 5 output features is a weighted sum of all 10 input features\n",
    "print(layer.weight.shape)  \n",
    "# biases\n",
    "# biases shape should be (out_features)\n",
    "# each of the 5 output features has a bias\n",
    "print(layer.bias.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 768])\n"
     ]
    }
   ],
   "source": [
    "# embedding layer \n",
    "# do lookup, from token id to a tensor\n",
    "# projection [vocab_size,] to [embedding_dim,]\n",
    "vocab_size = 20000 # tokenizer token count\n",
    "embed_dim = 768    # projection layer size\n",
    "layer = torch.nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "input_token_ids = [\n",
    "    [1,2],\n",
    "    [4,5],\n",
    "    [7,8],\n",
    "] # batch size = 3, vector size = 2\n",
    "output_tensor = layer(torch.tensor(input_token_ids))\n",
    "print(output_tensor.shape) # [3,2,768] ([batch_size, vector_size, embedding_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropout layer\n",
    "layer = torch.nn.Dropout(p=0.5)  # Dropout with probability of 0.5\n",
    "print(layer.forward(torch.randn(4, 4)))  # Apply dropout to a tensor  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convolutional layers\n",
    "layer = torch.nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3)  # 2D convolution\n",
    "layer = torch.nn.Conv1d(in_channels=1, out_channels=1, kernel_size=3)  # 1D convolution\n",
    "print(layer.weight.shape)  # Print weights of the convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pooling layers\n",
    "layer = torch.nn.MaxPool2d(kernel_size=2)  # Max pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recurrent layers\n",
    "torch.nn.RNN(input_size=10, hidden_size=20)  # RNN layer\n",
    "torch.nn.LSTM(input_size=10, hidden_size=20)  # LSTM layer\n",
    "torch.nn.GRU(input_size=10, hidden_size=20)  # GRU layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation functions\n",
    "# softmax function\n",
    "torch.nn.Softmax(dim=1) \n",
    "\n",
    "# sigmoid function\n",
    "torch.nn.Sigmoid()\n",
    "\n",
    "# relu function\n",
    "torch.nn.ReLU()\n",
    "\n",
    "# tanh function\n",
    "torch.nn.Tanh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.nn.CrossEntropyLoss loss: 0.4170299470424652\n",
      "torch.nn.BCELoss: 0.10536054521799088\n",
      "torch.nn.BCEWithLogitsLoss: 0.3156677782535553\n"
     ]
    }
   ],
   "source": [
    "# loss functions\n",
    "# mean squared error loss\n",
    "torch.nn.MSELoss()\n",
    "# cross entropy loss - multi-label \n",
    "# input: logits (before softmax), target (class index 0,1,2...)\n",
    "# processing: logits -> softmax -> loss\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "logits = torch.tensor([[2.0, 1.0, 0.1]])  # shape [1, 3], 3 classes\n",
    "target = torch.tensor([0])  # correct class = 0\n",
    "loss = loss_fn(logits, target)\n",
    "print(f\"torch.nn.CrossEntropyLoss loss: {loss}\")\n",
    "\n",
    "# binary cross entropy loss - 0/1 or multi-label\n",
    "# input: proba (after sigmoid), target (0 or 1)\n",
    "# processing: proba -> loss\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "pred = torch.tensor([0.9])   # already sigmoid probability\n",
    "target = torch.tensor([1.0]) # true label\n",
    "loss = loss_fn(pred, target)\n",
    "print(f\"torch.nn.BCELoss: {loss}\")\n",
    "\n",
    "# binary cross entropy loss - 0/1 or multi-label\n",
    "# input: logits (before sigmoid), target (0 or 1)\n",
    "# processing: logits -> sigmoid -> BCE\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "logits = torch.tensor([0.0, 2.0, -2.0])  # shape [3]\n",
    "targets = torch.tensor([0.0, 1.0, 0.0])  # shape [3]\n",
    "loss =loss_fn(logits, targets)\n",
    "print(f\"torch.nn.BCEWithLogitsLoss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.8.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
