{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention\n",
    "# Scaled Dot-Product Attention implementation\n",
    "# It's fundermental for multi-head attention\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, d_model, d_k, d_v):\n",
    "        super().__init__()\n",
    "        self.d_k = d_k\n",
    "        \n",
    "        # Linear projections for Q, K, V\n",
    "        # Q: Query\n",
    "        # K: Index to calculate relavent score between Q and V \n",
    "        # V: The actual content we want to match with\n",
    "        self.W_q = nn.Linear(d_model, d_k)\n",
    "        self.W_k = nn.Linear(d_model, d_k)\n",
    "        self.W_v = nn.Linear(d_model, d_v)\n",
    "        \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        # Linear projections\n",
    "        q = self.W_q(q)  # (batch_size, seq_len, d_k)\n",
    "        k = self.W_k(k)  # (batch_size, seq_len, d_k)\n",
    "        v = self.W_v(v)  # (batch_size, seq_len, d_v)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / np.sqrt(self.d_k)  # (batch_size, seq_len, seq_len)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "            \n",
    "        attention_weights = torch.softmax(scores, dim=-1)  # (batch_size, seq_len, seq_len)\n",
    "        output = torch.matmul(attention_weights, v)  # (batch_size, seq_len, d_v)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# Q&A\n",
    "# Q: Why k.transpose(-2, -1)?\n",
    "# A: We use transpose(-2, -1) to allows us to compute: QK^T to calculate attention score between Q and K\n",
    "\n",
    "# Q: What is attention score?\n",
    "# A: Attention score is the score of each position to other positions. You can think of it as the similarity between the query and the key.\n",
    "\n",
    "# Q: Why divide by np.sqrt(self.d_k)?\n",
    "# A: Divide by np.sqrt(self.d_k): avoid large values in dot products which lead to small gradients in softmax\n",
    "\n",
    "# Q: Why divide by np.sqrt(self.d_k), not self.d_k or self.d_k^2?\n",
    "# A: mathematical explanation: assume Q and K are random variables with mean 0 and variance 1, \n",
    "#    then the dot product of Q and K has mean 0 and variance d_k\n",
    "#    so we divide by np.sqrt(self.d_k) to normalize the variance to 1 again\n",
    "\n",
    "# Q: Why softmax?\n",
    "# A: Softmax is used to convert the attention scores into a probability distribution.\n",
    "#    It makes all outputs sum to 1 and each output is between 0 and 1.\n",
    "#    This is perfect for attention weights because We want to know \"how much attention\" (proportion) to pay to each position.\n",
    "#    Sigmoid not used because output sum != 1.\n",
    "#    ReLU not used because it does not normalize outputs and create a probability distribution.\n",
    "#    Tanh not used because it does not create a probability distribution and negative values does not make sense for attention weights.\n",
    "\n",
    "# Q: What is the output of the attention mechanism?\n",
    "# A: 2 outputs, main output and attention weights.\n",
    "#    Main output will be used for the next layer. It's a weighted sum of the values.\n",
    "#    Each position gets a new representation based on what it attended to.\n",
    "#    Attention weights are used to visualize the attention distribution.\n",
    "\n",
    "# Q: What are the q, k, v in input?\n",
    "# A: In self-attention (encoder), q, k,v all come from the same input sequence. They have same seq_len.\n",
    "#    In cross-attention (decoder), q comes from the decoder's input, k,v come from the encoder's output. They may have different seq_len.\n",
    "#    (Example: In machine translation, the decoder may have a different seq_len from the encoder)\n",
    "\n",
    "# Q: Why use three different projections?\n",
    "# A: Each projection(W_q, W_k, W_v) learns different aspects of the input. \n",
    "#    Having separate projections allows the model to learn different representations for different purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([64, 3, 4])\n",
      "Output shape: torch.Size([64, 3, 2])\n",
      "Attention weights shape: torch.Size([64, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "def test_attention():\n",
    "    # Create sample input\n",
    "    batch_size = 64\n",
    "    seq_len = 3\n",
    "    d_model = 4\n",
    "    d_k = 2\n",
    "    d_v = 2\n",
    "    \n",
    "    # Create random input tensors\n",
    "    x = torch.randn(batch_size, seq_len, d_model)\n",
    "    \n",
    "    # Initialize attention layer\n",
    "    attention = ScaledDotProductAttention(d_model, d_k, d_v)\n",
    "    \n",
    "    # Forward pass\n",
    "    output, attention_weights = attention(x, x, x)\n",
    "    \n",
    "    print(\"Input shape:\", x.shape)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    print(\"Attention weights shape:\", attention_weights.shape)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# Test the implementation\n",
    "output, attention_weights = test_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Head Attention implementation\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads  # dimension of each head\n",
    "        self.d_v = self.d_k  # typically d_v = d_k\n",
    "        \n",
    "        # Linear projections for Q, K, V\n",
    "        self.W_q = nn.Linear(d_model, d_model)  # project to d_model dimensions\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Output projection\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        self.attention = ScaledDotProductAttention(self.d_k, self.d_k, self.d_v)\n",
    "        \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        batch_size = q.size(0)\n",
    "        \n",
    "        # Linear projections and reshape for multi-head\n",
    "        q = self.W_q(q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        k = self.W_k(k).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        v = self.W_v(v).view(batch_size, -1, self.num_heads, self.d_v).transpose(1, 2)\n",
    "        \n",
    "        # Apply attention to each head\n",
    "        outputs = []\n",
    "        attention_weights = []\n",
    "        \n",
    "        for h in range(self.num_heads):\n",
    "            output, attn = self.attention(q[:, h], k[:, h], v[:, h], mask)\n",
    "            outputs.append(output)\n",
    "            attention_weights.append(attn)\n",
    "        \n",
    "        # Concatenate all heads\n",
    "        output = torch.cat(outputs, dim=0).view(batch_size, -1, self.d_model)\n",
    "        \n",
    "        # Final linear projection\n",
    "        output = self.W_o(output)\n",
    "        \n",
    "        # Average attention weights across heads\n",
    "        attention_weights = torch.stack(attention_weights).mean(dim=0)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# Q&A\n",
    "# Q: What is common value for d_model, d_k and d_v?\n",
    "# A: d_model is the dimension of the input and output vectors throughout the transformer architecture. \n",
    "#    In the original paper, d_model = 512. In BERT, d_model = 768(BASE) or 1024(LARGE). In GPT-2, d_model = 768(SMALL) to 4096(XL).\n",
    "#    d_k = d_v = d_model / h, where h is the number of heads. Use same dimension for query, key, and value.\n",
    "#    In typical transformer model, d_model = 512, h = 8, so d_k = d_v = 512 / 8 = 64.\n",
    "\n",
    "# Q: Why d_k = d_v = d_model / h?\n",
    "# A: We choose d_k = d_v = d_model / h because it allows multiple heads to work in parallel, each focusing on different aspects of the input.\n",
    "\n",
    "# Q: What's the purpose of multi-head?\n",
    "# A: Each head learns a different aspect of the input, so it can attend to different parts of the input.\n",
    "#    eg, head1 focuses on local dependencies, head2 focus on long-range dependencies, head3 focus on syntactic relationships, etc.\n",
    "#    This allows the model to attend to different parts of the input and learn different patterns.\n",
    "\n",
    "# Q: Why use linear layer for output projection\n",
    "# A: It helps information integration across different heads.\n",
    "#    The output projection adds learnable parameters that help the model learn how to best combine the information from different attention head.\n",
    "\n",
    "# Q: Why use average attention weights across heads?\n",
    "# A: We got an average aggregated attention weights across heads. It better represent the attention distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 3, 8])\n",
      "Output shape: torch.Size([2, 3, 8])\n",
      "Attention weights shape: torch.Size([2, 3, 3])\n",
      "\n",
      "Number of heads: 2\n",
      "Dimension per head: 4\n"
     ]
    }
   ],
   "source": [
    "# Test multi-head attention\n",
    "def test_multi_head_attention():\n",
    "    # Create sample input\n",
    "    batch_size = 2\n",
    "    seq_len = 3\n",
    "    d_model = 8  # must be divisible by num_heads\n",
    "    num_heads = 2  # number of heads\n",
    "    \n",
    "    # Create random input tensors\n",
    "    x = torch.randn(batch_size, seq_len, d_model)\n",
    "    \n",
    "    # Initialize multi-head attention layer\n",
    "    mha = MultiHeadAttention(d_model, num_heads)\n",
    "    \n",
    "    # Forward pass\n",
    "    output, attention_weights = mha(x, x, x)\n",
    "    \n",
    "    print(\"Input shape:\", x.shape)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    print(\"Attention weights shape:\", attention_weights.shape)\n",
    "    print(\"\\nNumber of heads:\", num_heads)\n",
    "    print(\"Dimension per head:\", d_model // num_heads)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# Test the implementation\n",
    "output, attention_weights = test_multi_head_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 64])\n",
      "torch.Size([2, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "# self-attention\n",
    "# Self-attention is a mechanism where each position in a sequence attends to \n",
    "# all positions in the same sequence to compute a representation.\n",
    "# 1. In self-attention, we use same input for q, k, v\n",
    "# 2. Each position in the sequence can look at all other positions in the sequence\n",
    "# 3. Helps capture relationship between different parts of the sequence\n",
    "\n",
    "batch_size = 2\n",
    "seq_len = 4\n",
    "d_model = 512\n",
    "d_k = 64\n",
    "d_v = 64\n",
    "\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "attention = ScaledDotProductAttention(d_model, d_k, d_v)\n",
    "output, attention_weights = attention(x, x, x)\n",
    "print(output.shape)\n",
    "print(attention_weights.shape)\n",
    "\n",
    "# Q&A\n",
    "# Q: What is the purpose of self-attention?\n",
    "# A: It allows the model to attend to different parts of the input sequence to compute a representation.\n",
    "#    It helps capture relationship between different parts of the sequence.\n",
    "\n",
    "# Q: How is self-attention used in the transformer model?\n",
    "# A: In encoder, each layer contains a self-attention mechanism.\n",
    "#    In decoder, each layer contains a self-attention mechanism and a cross-attention mechanism. \n",
    "#    A masking is used to prevent positions from attending to subsequent positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 64])\n",
      "torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "# cross-attention\n",
    "# Cross-attention is a mechanism where the decoder attends to the encoder's output, \n",
    "# allowing it to focus on relevant parts of the input sequence when generating the output sequence.\n",
    "# 1. In cross-attention, q comes from decoder, k and v come from encoder\n",
    "# 2. This allows the decoder to \"look at\" the encoder's output when generating each token\n",
    "# 3. It's like the decoder asking \"what parts of the input should I focus on now\"\n",
    "\n",
    "batch_size = 2\n",
    "seq_len_input = 4\n",
    "seq_len_output = 6\n",
    "d_model = 512\n",
    "d_k = 64\n",
    "d_v = 64\n",
    "\n",
    "x = torch.randn(batch_size, seq_len_output, d_model)\n",
    "k = torch.randn(batch_size, seq_len_input, d_model)\n",
    "v = torch.randn(batch_size, seq_len_input, d_model)\n",
    "attention = ScaledDotProductAttention(d_model, d_k, d_v)\n",
    "output, attention_weights = attention(x, k, v)\n",
    "print(output.shape)\n",
    "print(attention_weights.shape)\n",
    "\n",
    "# Q&A\n",
    "# Q: What happens when seq_len_output is different from seq_len_input?\n",
    "# A: It will be okay because we have k and v from encoder, which has same length as seq_len_input.\n",
    "\n",
    "# Q: How does the length of output sequence decided? \n",
    "# A: After predicting the next token = [END], we can stop the generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Self-attention layer\n",
    "        self.self_attention = MultiHeadAttention(d_model, num_heads)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-attention block\n",
    "        attn_output, _ = self.self_attention(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        # Feed-forward block\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Self-attention layer\n",
    "        self.self_attention = MultiHeadAttention(d_model, num_heads)\n",
    "        \n",
    "        # Cross-attention layer\n",
    "        self.cross_attention = MultiHeadAttention(d_model, num_heads)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):\n",
    "        # Self-attention block\n",
    "        attn_output, _ = self.self_attention(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        # Cross-attention block\n",
    "        attn_output, _ = self.cross_attention(x, encoder_output, encoder_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        \n",
    "        # Feed-forward block\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict next token"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
