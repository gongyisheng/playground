{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention\n",
    "# Scaled Dot-Product Attention implementation\n",
    "# It's fundermental for multi-head attention\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, d_model, d_k, d_v):\n",
    "        super().__init__()\n",
    "        self.d_k = d_k\n",
    "        \n",
    "        # Linear projections for Q, K, V\n",
    "        # Q: Query\n",
    "        # K: Index to calculate relavent score between Q and V \n",
    "        # V: The actual content we want to match with\n",
    "        self.W_q = nn.Linear(d_model, d_k)\n",
    "        self.W_k = nn.Linear(d_model, d_k)\n",
    "        self.W_v = nn.Linear(d_model, d_v)\n",
    "        \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        # Linear projections\n",
    "        q = self.W_q(q)  # (batch_size, seq_len, d_k)\n",
    "        k = self.W_k(k)  # (batch_size, seq_len, d_k)\n",
    "        v = self.W_v(v)  # (batch_size, seq_len, d_v)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / np.sqrt(self.d_k)  # (batch_size, seq_len, seq_len)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "            \n",
    "        attention_weights = torch.softmax(scores, dim=-1)  # (batch_size, seq_len, seq_len)\n",
    "        output = torch.matmul(attention_weights, v)  # (batch_size, seq_len, d_v)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# Q&A\n",
    "# Q: Why k.transpose(-2, -1)?\n",
    "# A: We use transpose(-2, -1) to allows us to compute: QK^T to calculate attention score between Q and K\n",
    "\n",
    "# Q: What is attention score?\n",
    "# A: Attention score is the score of each position to other positions. You can think of it as the similarity between the query and the key.\n",
    "\n",
    "# Q: Why divide by np.sqrt(self.d_k)?\n",
    "# A: Divide by np.sqrt(self.d_k): avoid large values in dot products which lead to small gradients in softmax\n",
    "\n",
    "# Q: Why divide by np.sqrt(self.d_k), not self.d_k or self.d_k^2?\n",
    "# A: mathematical explanation: assume Q and K are random variables with mean 0 and variance 1, \n",
    "#    then the dot product of Q and K has mean 0 and variance d_k\n",
    "#    so we divide by np.sqrt(self.d_k) to normalize the variance to 1 again\n",
    "\n",
    "# Q: Why softmax?\n",
    "# A: Softmax is used to convert the attention scores into a probability distribution.\n",
    "#    It makes all outputs sum to 1 and each output is between 0 and 1.\n",
    "#    This is perfect for attention weights because We want to know \"how much attention\" (proportion) to pay to each position.\n",
    "#    Sigmoid not used because output sum != 1.\n",
    "#    ReLU not used because it does not normalize outputs and create a probability distribution.\n",
    "#    Tanh not used because it does not create a probability distribution and negative values does not make sense for attention weights.\n",
    "\n",
    "# Q: What is the output of the attention mechanism?\n",
    "# A: 2 outputs, main output and attention weights.\n",
    "#    Main output will be used for the next layer. It's a weighted sum of the values.\n",
    "#    Each position gets a new representation based on what it attended to.\n",
    "#    Attention weights are used to visualize the attention distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([64, 3, 4])\n",
      "Output shape: torch.Size([64, 3, 2])\n",
      "Attention weights shape: torch.Size([64, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "def test_attention():\n",
    "    # Create sample input\n",
    "    batch_size = 64\n",
    "    seq_len = 3\n",
    "    d_model = 4\n",
    "    d_k = 2\n",
    "    d_v = 2\n",
    "    \n",
    "    # Create random input tensors\n",
    "    x = torch.randn(batch_size, seq_len, d_model)\n",
    "    \n",
    "    # Initialize attention layer\n",
    "    attention = ScaledDotProductAttention(d_model, d_k, d_v)\n",
    "    \n",
    "    # Forward pass\n",
    "    output, attention_weights = attention(x, x, x)\n",
    "    \n",
    "    print(\"Input shape:\", x.shape)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    print(\"Attention weights shape:\", attention_weights.shape)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# Test the implementation\n",
    "output, attention_weights = test_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Head Attention implementation\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads  # dimension of each head\n",
    "        self.d_v = self.d_k  # typically d_v = d_k\n",
    "        \n",
    "        # Linear projections for Q, K, V\n",
    "        self.W_q = nn.Linear(d_model, d_model)  # project to d_model dimensions\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Output projection\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        self.attention = ScaledDotProductAttention(self.d_k, self.d_k, self.d_v)\n",
    "        \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        batch_size = q.size(0)\n",
    "        \n",
    "        # Linear projections and reshape for multi-head\n",
    "        q = self.W_q(q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        k = self.W_k(k).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        v = self.W_v(v).view(batch_size, -1, self.num_heads, self.d_v).transpose(1, 2)\n",
    "        \n",
    "        # Apply attention to each head\n",
    "        outputs = []\n",
    "        attention_weights = []\n",
    "        \n",
    "        for h in range(self.num_heads):\n",
    "            output, attn = self.attention(q[:, h], k[:, h], v[:, h], mask)\n",
    "            outputs.append(output)\n",
    "            attention_weights.append(attn)\n",
    "        \n",
    "        # Concatenate all heads\n",
    "        output = torch.cat(outputs, dim=0).view(batch_size, -1, self.d_model)\n",
    "        \n",
    "        # Final linear projection\n",
    "        output = self.W_o(output)\n",
    "        \n",
    "        # Average attention weights across heads\n",
    "        attention_weights = torch.stack(attention_weights).mean(dim=0)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# Q&A\n",
    "# Q: What is common value for d_model, d_k and d_v?\n",
    "# A: d_model is the dimension of the input and output vectors throughout the transformer architecture. \n",
    "#    In the original paper, d_model = 512. In BERT, d_model = 768(BASE) or 1024(LARGE). In GPT-2, d_model = 768(SMALL) to 4096(XL).\n",
    "#    d_k = d_v = d_model / h, where h is the number of heads. Use same dimension for query, key, and value.\n",
    "#    In typical transformer model, d_model = 512, h = 8, so d_k = d_v = 512 / 8 = 64.\n",
    "\n",
    "# Q: Why d_k = d_v = d_model / h?\n",
    "# A: We choose d_k = d_v = d_model / h because it allows multiple heads to work in parallel, each focusing on different aspects of the input.\n",
    "\n",
    "# Q: What's the purpose of multi-head?\n",
    "# A: Each head learns a different aspect of the input, so it can attend to different parts of the input.\n",
    "#    eg, head1 focuses on local dependencies, head2 focus on long-range dependencies, head3 focus on syntactic relationships, etc.\n",
    "#    This allows the model to attend to different parts of the input and learn different patterns.\n",
    "\n",
    "# Q: Why use linear layer for output projection\n",
    "# A: It helps information integration across different heads.\n",
    "#    The output projection adds learnable parameters that help the model learn how to best combine the information from different attention head.\n",
    "\n",
    "# Q: Why use average attention weights across heads?\n",
    "# A: We got an average aggregated attention weights across heads. It better represent the attention distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 3, 8])\n",
      "Output shape: torch.Size([2, 3, 8])\n",
      "Attention weights shape: torch.Size([2, 3, 3])\n",
      "\n",
      "Number of heads: 2\n",
      "Dimension per head: 4\n"
     ]
    }
   ],
   "source": [
    "# Test multi-head attention\n",
    "def test_multi_head_attention():\n",
    "    # Create sample input\n",
    "    batch_size = 2\n",
    "    seq_len = 3\n",
    "    d_model = 8  # must be divisible by num_heads\n",
    "    num_heads = 2  # number of heads\n",
    "    \n",
    "    # Create random input tensors\n",
    "    x = torch.randn(batch_size, seq_len, d_model)\n",
    "    \n",
    "    # Initialize multi-head attention layer\n",
    "    mha = MultiHeadAttention(d_model, num_heads)\n",
    "    \n",
    "    # Forward pass\n",
    "    output, attention_weights = mha(x, x, x)\n",
    "    \n",
    "    print(\"Input shape:\", x.shape)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    print(\"Attention weights shape:\", attention_weights.shape)\n",
    "    print(\"\\nNumber of heads:\", num_heads)\n",
    "    print(\"Dimension per head:\", d_model // num_heads)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# Test the implementation\n",
    "output, attention_weights = test_multi_head_attention()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
