{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "# ref: https://huggingface.co/docs/transformers/en/main_classes/tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 31414, 623, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 2387, 766, 16, 1560, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 5625, 16, 18862, 46836, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode text\n",
    "model_name = 'distilbert/distilroberta-base'\n",
    "max_length = 32\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "text = [\"Hello World\", \"My name is Tom\", \"Today is wednesday\"]\n",
    "\n",
    "encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=max_length)\n",
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[max_length] shape: (3, 16)\n",
      "[longest] shape: (3, 6)\n"
     ]
    }
   ],
   "source": [
    "# use different padding strategies\n",
    "model_name = 'distilbert/distilroberta-base'\n",
    "max_length = 16\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def print_shape(encoding, label):\n",
    "    shape = (len(encoding[\"input_ids\"]), len(encoding[\"input_ids\"][0]))\n",
    "    print(f\"[{label}] shape:\", shape)\n",
    "\n",
    "text = [\"Hello World\", \"My name is Tom\", \"Today is wednesday\"]\n",
    "\n",
    "encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=max_length)\n",
    "print_shape(encoding, \"max_length\")\n",
    "\n",
    "encoding = tokenizer(text, padding=\"longest\", truncation=True, max_length=max_length)\n",
    "print_shape(encoding, \"longest\")\n",
    "\n",
    "encoding = tokenizer(text, padding=False, truncation=True, max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 31414, 623, 2]]\n",
      "right: [[0, 31414, 623, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "# padding side: left or right\n",
    "model_name = 'distilbert/distilroberta-base'\n",
    "max_length = 16\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "text = [\"Hello World\"]\n",
    "\n",
    "encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=max_length, padding_side='left')\n",
    "print(\"left:\", encoding['input_ids'])\n",
    "\n",
    "encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=max_length, padding_side='right')\n",
    "print(\"right:\", encoding['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 713, 16, 41, 1246, 9, 10, 2788, 14, 16, 1341, 8787, 8, 1411, 1684, 5, 4532, 1220, 8135, 5933, 13, 1209, 11760, 3092, 4, 1773, 52, 64, 75, 3993, 42, 1445, 2788, 88, 5, 1421, 396, 43064, 1295, 24, 6, 52, 3253, 43064, 1258, 7, 1306, 14, 5, 2788, 10698, 624, 5, 1421, 18, 4532, 8135, 5933, 3000, 4, 2]\n",
      "[0, 713, 16, 41, 1246, 9, 10, 2]\n"
     ]
    }
   ],
   "source": [
    "# truncation\n",
    "model_name = 'distilbert/distilroberta-base'\n",
    "max_length = 8\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "text = (\"This is an example of a text that is quite lengthy and goes beyond \"\n",
    "        \"the maximum allowed input length for Qwen models. Since we can't \"\n",
    "        \"feed this entire text into the model without truncating it, we apply \"\n",
    "        \"truncation to ensure that the text fits within the model's maximum input \"\n",
    "        \"length limit.\")\n",
    "\n",
    "encoding = tokenizer(text, truncation=False, max_length=max_length)\n",
    "print(encoding[\"input_ids\"])\n",
    "\n",
    "encoding = tokenizer(text, truncation=True, max_length=max_length)\n",
    "print(encoding[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# return tensorflow / pytorch / numpy tensor\n",
    "# return_tensors (str or TensorType, optional) â€” If set, will return tensors instead of list of python integers. Acceptable values are:\n",
    "# 'tf': Return TensorFlow tf.constant objects.\n",
    "# 'pt': Return PyTorch torch.Tensor objects.\n",
    "# 'np': Return Numpy np.ndarray objects.\n",
    "\n",
    "model_name = 'distilbert/distilroberta-base'\n",
    "max_length = 16\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "text = [\"Hello World\"]\n",
    "\n",
    "encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors='tf')\n",
    "print(type(encoding[\"input_ids\"]))\n",
    "\n",
    "encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors='pt')\n",
    "print(type(encoding[\"input_ids\"]))\n",
    "\n",
    "encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors='np')\n",
    "print(type(encoding[\"input_ids\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
