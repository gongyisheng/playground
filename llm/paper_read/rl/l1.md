# L1
title: L1: Controlling How Long A Reasoning Model Thinks With Reinforcement Learning  
url: https://arxiv.org/abs/2503.04697  

## Key Takeaways
- Length Controlled Policy Optimization (LCPO): a RL policy which sets think length in prompt, use correctness of result + length requirement satisfactory as objective to control reasoning model's think length.

## Method
```
LCPO-Exact: 
    - add prompt at end: Think for exactly {num} tokens.
    - RL reward func: r(y, y_gold, n_gold) = I(y = y_gold) - α · |n_gold - n_y| 
        (I(y = y_gold): indicator func, 0 or 1, α: positive scalar to determine punishment scale)
LCPO-Max: 
    - add prompt at end: Think for maximum {num} tokens.
    - RL reward func: r(y, y_gold, n_gold) = I(y = y_gold) · clip(α · (n_gold - n_y) + δ, 0, 1) 
        (I(y = y_gold): indicator func, 0 or 1, clip(x, 0, 1): limits the value to [0,1], δ: base reward shift, set to 0.5 here.)
    - Objective: 1. get right result 2. use as less token as you can
```

## Experiment
```
Model: 
    - Deepseek-R1-Distill-Qwen-1.5B
    - DeepScaleR-1.5B-Preview
    - DeepScaleR-1.5B-Preview-4K
    - S1 (truncate or add special token "wait")
Dataset: DeepScaleR-Preview-Datase
GRPO Training, lr=1e-6, batch_size=128, step=700, VeRL framework, α = 0.0003
L1-Max is finetuned on L1-Exact, so that the think length instruction following is ensured
```

## Result
- L1 significantly outperforms other length-control models
- L1 generalizes effectively to out-of-domain (OOD) tasks
- L1 follows length constraints with high precision.
- L1-Max is better than L1-Exact

## Other Findings
- L1 can intelligently adapt its CoT within length constraints without truncating
- L1 can generate high quality CoT of vary length
- Observe model performance improves linearly with log of reasoning length. 
- Observe model has higher length deviation in OOD tasks (20%-40%)