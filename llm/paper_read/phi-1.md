# Phi-1
title: Textbooks Are All You Need  
url: https://arxiv.org/abs/2306.11644  

## Key Takeaways
- with help of high quality textbook level data, small models can achieve same performance as large models with much less data
- novel methods of building training set, synthesis data, gpt as grader

## Design
1. idea
    ```
    current training data (THe Stack, StackOverflow and CodeContes) is not optimal for teaching LLM how to code
    - not self-contained, depends on other modules / files
    - not meaningful, eg some constants code and setting param code
    - code contains lgorithmic logic are poorly documented, deeply buried and complex to understand
    - data skew, contain too many duplicated cases specific to some topic / use case
    Even human cannot learn how to code based on these training data. 
    Maybe LLM need a textbook level quality of data to learn how to code.
    ```
2. training dataset
    ```
    a. filtering current dataset using GPT-4 + random forest (7b token)
        GPT-4 here is used for annotating a small part of data to avoid human labeling
        GPT-4 prompt: determine its educational value for a student whose goal is to learn basic coding concepts
        Then use random forest classifier based on codegen model embedding
    b. synthetic textbook from GPT-3.5 (1b token)
        dataset should be diverse and non-repetitive
        diverse: cover big range of concepts, skills, difficulty, style.
        used GPT-3.5 to generate the dataset, focus on topics promote reasoning and algorithm skills
    c. synthetic exercise (180m token)
        exercise and answers, generated by GPT-3.5, diversity is achieved based on constraint on names
        this dataset is used as finetune dataset, extensive data pruning is used to make sure it not cover with evaluation set
        - N-gram overlap
        - embedding similarity
        - match rate of AST edit distance (this works best, detects examples like only function name changed but code is same)
    ```
3. training
    ```
    1.3B phi-1 
    350M phi-1-small

    fp16, AdamW, linear-warmup-linear-decay, dropout=0.1 

    pretrain: batch size=1024, 36000 steps (8 epoch), lr=1e-3, warm up=750, decay=0.1, pick checkpoint-24000
    finetune: batch size=256, 6000 steps, lr=1e-4, warm up=50, decay=0.01

    pretrain: 4d on 8 A100
    finetune: 7h on 8 A100
    ```

## result
1. even only use filtered dataset can significantly boost model performance
2. finetuning unlocks model ability to use external libs 
3. model achieves similar performance on coding tasks as big models. phi-1 is better than phi-1-small

## Other Findings
1. If LLM is simply prompted, it only generates very homogeneous and redundant dataset. This is because LLMs tend to follow the most probable or common paths given their training data and their priors, and they lack the creativity or the incentive to explore alternative or novel ways of generating code.
2. Ways to avoid #1: provide some constraints (topics, target audience, function names) of the generated textbook. 
3. GPT-4 as grader: make eval focus on logic, not only the result, make it more like human interviews. prompt: evaluate a studentâ€™s solution first in a short verbal evaluation followed by grades from 0 to 10