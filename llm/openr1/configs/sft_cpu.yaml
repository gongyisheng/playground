# Model 
model_name_or_path: open-r1/Qwen2.5-Math-7B-RoPE-300k

# Dataset 
dataset_name: open-r1/Mixture-of-Thoughts
dataset_config: all

# Tokenizer 
eos_token: '<|im_end|>'
dataset_num_proc: 8

# Training 
learning_rate: 4.0e-5
num_train_epochs: 5
max_length: 32768
per_device_train_batch_size: 2
gradient_checkpointing: true
bf16: false
warmup_ratio: 0.1

# Wandb 
report_to: wandb
wandb_project: qwen2.5-math-7b-sft

# Output 
output_dir: /home/yisheng/Documents/replicate/openr1/qwen2.5-math-7b-sft
